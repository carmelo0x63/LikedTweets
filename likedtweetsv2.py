#!/usr/bin/env python3
# Script consuming Twitter API to retrieve the liked tweets for a specific user incrementally
# author: Carmelo C
# email: carmelo.califano@gmail.com
# history, date format ISO 8601:
#  2022-08-04  Moved to Twitter API v2

# External modules/dependencies
import argparse                # Parser for command-line options, arguments and sub-commands
import json                    # JSON encoder and decoder
import os                      # Miscellaneous operating system interfaces
import requests                # HTTP library for Python
import shutil                  # High-level file operations
import subprocess              # Subprocess management
import sys                     # System-specific parameters and functions
from datetime import datetime  # Basic date and time types
from json2html import *        # Python wrapper for JSON to HTML-Table convertor

# Global settings
__version__ = '3.0'
__build__ = '20220804'
TIMESTAMP = datetime.now().strftime('%Y-%m-%d-%H')
ARCHIVEDIR = '_archive'
# Default fields: id, text
# Additional fields
expansions = 'author_id'
tweet_fields = 'author_id,created_at,entities'
user_fields = 'id,name,username'
query_params = dict([
    ('expansions', expansions),
    ('tweet.fields', tweet_fields),
    ('user.fields', user_fields),
#    ('max_results', 5)  ## DEBUG, comment out in final version
])


def createUrl(id):
    """
    createUrl is an URL generator based on v2 endpoint + Twitter ID
    Args:
    - id (string): Twitter IDs are 64-bit integers, treated as a string here
    """
    url = f'https://api.twitter.com/2/users/{id}/liked_tweets'
    return url


def readConf(name):
    """
    readConf() reads the application's configuration from an external file.
    The file is JSON-formatted and contains:
    - the twitter ID
    - the OAuth2 bearer token
    - the last timestamp
    Args:
    - name (string): Twitter user name
    """
    try:
        with open(name + '_configv2.json', 'r') as config_in:
            config_json = json.load(config_in)
        if ISVERBOSE: print('[+] Config file found for ' + name)
        if config_json['BEARER'] == '':
            print('[-] Bearer token empty!')
            print('[-] Quitting!', end = '\n\n')
            sys.exit(50)  # ERROR: empty Bearer token
        else:
            if ISVERBOSE: print('[+] Bearer token found!')
        if ISVERBOSE:
            beautify_last_timestamp = config_json['last_timestamp'] or 'EMPTY'
            print('[+] Last timestamp is: ' + beautify_last_timestamp)
        return config_json
    except FileNotFoundError:
        print('[-] Config file not found for ' + name)
        print('[-] Quitting!', end = '\n\n')
        sys.exit(20)  # ERROR: wrong user name / config file not found


def connect2Endpoint(url, bearer_token, query_params):
    """
    connect2Endpoint() consumes Twitter v2 API "liked_tweets" endpoint. The response, once converted to JSON,
    consists of the following keys:
    - data (list of dicts): each element contains the fundamental info about the tweet
    - includes (dict): optional, its contents depends on the chosen expansion(s)
    - meta (dict): contains useful info such as result_count, next_token, previous_token
    Args:
    - url (string): the endpoint, generated by createUrl()
    - bearer_token (string): fetched from the configuration file
    - query_params (dict): global setting defined at the beginning of this script
    """
    headers = {'Authorization': f'Bearer {bearer_token}'}
    response = requests.request('GET', url, headers = headers, params = query_params)
    if response.status_code != 200:
        print('[-] An error has occurred')
        print(f'[-] HTTP status code = {response.status_code}')
        print(f'[-] HTTP reason = {response.reason}')
        print('[-] Quitting!', end = '\n\n')
        sys.exit(response.status_code)
    else:
        if ISVERBOSE: print(f'[+] HTTP status code = {response.status_code}')
    return response.json()


def mergeExpansions(data_json, includes_json):
    """
    mergeExpansion() parses 'response_json' for returned tweets, it searches the 'include' key to match 'tweet/author_id' and 'users/id'.
    Once that's done, it merges both dictionaries into one record comprised of:
    - id, text (from 'data', default values)
    - author_id, created_at (from 'data', optional values as per 'tweet.fields')
    - name, username (from 'includes', defined by 'expansions')
    Args:
    - data_json (dict): JSON-formatted output returned from querying the endpoint
    - includes_json (dict): JSON-formatted output returned from querying the endpoint
    """
    for tweet in data_json:
        author = tweet['author_id']
        for user in includes_json:
            if user['id'] == author:
                tweet.update({'author_name': user['name'], 'author_handle': user['username']})
    return data_json


def saveData(name, output_json):
    """
    saveData() writes to disk the full list of tweets
    Args:
    - name (string): Twitter username
    - output_json (list): list of tweets containing both 'data' and 'includes'
    """
    if ISVERBOSE: print('[!] Storing liked_tweets to local file')
    with open(name + '_likedtweets_' + TIMESTAMP + '.json', 'w') as output_file:
        json.dump(output_json, output_file)


def updateConf(name, config_json):
    """
    updateConf() updates the external configuration file with the last timestamp
    Args:
    - config_json (dict): contents of the configuration file
    - name (string): Twitter user name
    """
    if ISVERBOSE: print('[!] Updating configuration file')
    config_json.update(last_timestamp = TIMESTAMP)
    with open(name + '_configv2.json', 'w') as config_out:
        json.dump(config_json, config_out)


def convert2HTML(tweets_json, name, old_ts):
    """
    convert2HTML() converts the JSON file into a table-based HTML file
    Args:
    - tweets_json (list): list of tweets
    - name (string): Twitter user name
    - old_ts (string): previously saved timestamp
    """
    tweets_json_out = []
    tweets_length = len(tweets_json)

    for index in range(0, tweets_length):
        tweet = {"ROW": "", "USER INFO": "", "TWEET INFO": "", "FULL TEXT": "", "URL": ""}
        tweet.update({"ROW": index})
        tweet.update({"USER INFO": {"USER_ID": tweets_json[index]['author_id'], "USER_NAME": tweets_json[index]['author_name'], "USER_HANDLE": tweets_json[index]['author_handle']}})
        tweet.update({"TWEET INFO": {"TWEET_ID": tweets_json[index]['id'], "TWEET_DATE": tweets_json[index]['created_at']}})
        tweet.update({"FULL TEXT": tweets_json[index]['text']})
        try:
            tweet.update({"URL": tweets_json[index]['entities']['urls'][0]['expanded_url']})
        except:
            tweet.update({"URL": "N/A"})
        tweets_json_out.append(tweet)

    index_out = json2html.convert(json = tweets_json_out)

    with open(name + '_index_' + TIMESTAMP + '.html', 'w') as html_out:
        html_out.write(index_out)

    print('[+] Conversion to HTML done, processed ' + str(tweets_length) + ' records')
    if ISVERBOSE:
        print('[+] New file: ' + name + '_index_' + TIMESTAMP + '.html saved to disk')

    subprocess.check_output(['rm', '-f', name + '_index_latest.html'])
    subprocess.check_output(['ln', name + '_index_' + TIMESTAMP + '.html', name + '_index_latest.html'])
    if ISVERBOSE:
        print('[+] New file: ' + name + '_index_' + TIMESTAMP + '.html linked to LATEST')


def archiveFile(name, old_ts):
    """
    archiveFile() obsoletes old files by moving them to ARCHIVEDIR
    Args:
    - name (string): Twitter user name
    - old_ts (string): previously saved timestamp
    """
    if ISVERBOSE: print('[!] Archiving obsolete files')
    files = [name + '_likedtweets_' + old_ts + '.json', name + '_index_' + old_ts + '.html']
    for file in files:
        if os.path.isfile(file):
            if os.path.isdir(ARCHIVEDIR):
                shutil.move(file, ARCHIVEDIR + '/' + file)
                if ISVERBOSE:
                    print('[+] Archived file: ' + file)
            else:
                print('[-] "' + ARCHIVEDIR + '" is not present!')
                sys.exit(60)  # ERROR: archive directory not found
        else:
            print('[!] "' + file + '" is not present, skipping!')


def main():
    """
    main() handles the input (through argparse), an appropriate logic is implemented based on the input arguments
    """
    parser = argparse.ArgumentParser(description = 'Consumes Twitter API to retrieve the liked tweets incrementally, version ' + __version__ + ', build ' + __build__ + '.')
    parser.add_argument('-v', '--verbose', action = 'store_true', help = 'Print extended information')
    parser.add_argument('-V', '--Version', action = 'version', version = '%(prog)s {version}'.format(version=__version__))
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-g', '--get', metavar = '<User name>', default = '', type = str, help = 'User name or Twitter handle (w/o @)')
#    group.add_argument('-p', '--print', metavar = '<User name>', default = '', type = str, help = 'Pretty print local JSON archive to screen')
    group.add_argument('-t', '--tohtml', metavar = '<User name>', default = '', type = str, help = 'Convert local JSON archive to HTML')

    # In case of no arguments help message is shown
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(10)  # ERROR: no arguments
    else:
        args = parser.parse_args() # parse command line

    """
    First off, <name>_configv2.json is read to fetch the user's Twitter ID and any tokens
    Note: "get", "print", and "tohtml" arguments are mutually exclusive
    <User name>'s default value is the empty string, if <User name> has a non-empty value it must come from get/print/tohtml
    """
#    twitter_name = args.get + args.print + args.tohtml
    twitter_name = args.get + args.tohtml
    global ISVERBOSE
    ISVERBOSE = args.verbose
    if twitter_name != '':
        config_json = readConf(twitter_name)
    else:
        print('[-] User ID is empty!', end = '\n\n')
        sys.exit(30)  # ERROR: user ID is an empty string

    # Once an <User name> has been provided we go on by assembling the necessary variables
    twitter_id = config_json['twitter_id']
    url = createUrl(twitter_id)
    bearer_token = config_json['BEARER']
    last_timestamp = config_json['last_timestamp']

    # If "tohtml" mutually exclusive option is chosen we do the same as with "print"
    if args.tohtml:
        try:
            if ISVERBOSE: print('[+] Generating HTML output for user ' + twitter_name)
            filename = twitter_name + '_likedtweets_' + last_timestamp + '.json'
            with open(filename, 'r') as archive_in:
                archive_json = json.load(archive_in)
            convert2HTML(archive_json, twitter_name, last_timestamp)
            sys.exit(0)
        except FileNotFoundError:
            print('[-] Local archive ' + filename + ' not found')
            print('[-] Quitting!', end = '\n\n')
            sys.exit(40)  # ERROR: local archive not found
    # Once "tohtml" is done the script exits in a controlled fashion

    """
    At this stage a loop is triggered that queries the API endpoint until the response is empty
    for info, see [Pagination](https://developer.twitter.com/en/docs/twitter-api/pagination)
    at the end of the loop, "output_list" will bear a `list of dict`, treated as a JSON file
    each list element is a single tweet, enriched by merging info from both returned 'data' and 'includes'
    """
    count = records = 0
    next_token = 'dummy'  # value used only once, to set off the 'while' loop
    output_list = []
    while next_token:
        count += 1
        response_json = connect2Endpoint(url, bearer_token, query_params)
        if ISVERBOSE: print('[!] Iteration = ' + str(count) + ' with next_token = \'' + next_token + '\'')

        result_count = response_json['meta']['result_count']
        if ISVERBOSE: print('[!] Fetched ' + str(result_count) + ' records')
        records += result_count
        if ISVERBOSE: print('[!] Partial count: ' + str(records) + ' records thus far...')

        if result_count == 0:
            if ISVERBOSE:
                print('[!] No data returned')
                print('[!] Last page reached')
            print('[+] Operation completed, fetched ' + str(records) + ' records')
            break
        else:
            if ISVERBOSE:
                print('data: ')
                print(response_json['data'])
                print('includes: ')
                print(response_json['includes']['users'], end = '\n\n')

            merged_json = mergeExpansions(response_json['data'], response_json['includes']['users'])
            if ISVERBOSE:
                print('merged data: ')
                print(merged_json, end = '\n\n')

            output_list += merged_json
            if ISVERBOSE:
                print('data length = ' + str(len(response_json['data'])))
                print('current output len = ' + str(len(output_list)))
                print('meta = ' + str(response_json['meta']), end = '\n\n')

            next_token = response_json['meta']['next_token']
            query_params.update([('pagination_token', next_token)])

    # Finally, some manipulation occurs of the output files
    saveData(twitter_name, output_list)
    updateConf(twitter_name, config_json)
    if last_timestamp != TIMESTAMP:
        archiveFile(twitter_name, last_timestamp or 'EMPTY')

if __name__ == '__main__':
    main()

